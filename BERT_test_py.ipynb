{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_test.py",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBSHztJVGKELvNEZ0PrwZc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diverhaze/Overton_Pipeline/blob/main/BERT_test_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIs42B7zyXKp"
      },
      "source": [
        "Installer für BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fS67nc4seSa"
      },
      "source": [
        "pip install germansentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLLT6e7Kyas2"
      },
      "source": [
        "## Leicht modifizierter BERT Code  \n",
        "\n",
        "Modifizierung des generierten Outputs. Anstatt nur labels ausgegeben zu bekommen werden weitere Informationen ausgegeben. Diese beinhalten:  \n",
        "* tensor logits\n",
        "* argmax\n",
        "* labels\n",
        "\n",
        "\n",
        "**Hinweis:** Cuda erhöht die Performance signifikant, wenn eine GPU Unterstützung vorhanden ist, ist es äußerst empfehlenswert diese auch zu nutzen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTwq0GeJsie3"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from typing import List\n",
        "import torch\n",
        "import re\n",
        "\n",
        "\n",
        "class SentimentModel2():\n",
        "    def __init__(self, model_name: str = \"oliverguhr/german-sentiment-bert\"):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = 'cuda'\n",
        "        else:\n",
        "            self.device = 'cpu'\n",
        "\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        self.clean_chars = re.compile(r'[^A-Za-züöäÖÜÄß ]', re.MULTILINE)\n",
        "        self.clean_http_urls = re.compile(r'https*\\S+', re.MULTILINE)\n",
        "        self.clean_at_mentions = re.compile(r'@\\S+', re.MULTILINE)\n",
        "\n",
        "    def predict_sentiment(self, texts: List[str]) -> List[str]:\n",
        "        output = []             # modifiziert\n",
        "\n",
        "        texts = [self.clean_text(text) for text in texts]\n",
        "        # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
        "        # limit number of tokens to model's limitations (512)\n",
        "        input_ids = self.tokenizer.batch_encode_plus(texts, padding=True, add_special_tokens=True, truncation=True)\n",
        "        input_ids = torch.tensor(input_ids[\"input_ids\"])\n",
        "        input_ids = input_ids.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids)\n",
        "\n",
        "        output.append(logits)   # modifiziert\n",
        "\n",
        "        label_ids = torch.argmax(logits[0], axis=1)\n",
        "        output.append(label_ids)# modifiziert\n",
        "\n",
        "        labels = [self.model.config.id2label[label_id] for label_id in label_ids.tolist()]\n",
        "        output.append(labels)   # modifiziert\n",
        "\n",
        "        return output           # modifiziert\n",
        "\n",
        "    def replace_numbers(self, text: str) -> str:\n",
        "        return text.replace(\"0\", \" null\").replace(\"1\", \" eins\").replace(\"2\", \" zwei\") \\\n",
        "            .replace(\"3\", \" drei\").replace(\"4\", \" vier\").replace(\"5\", \" fünf\") \\\n",
        "            .replace(\"6\", \" sechs\").replace(\"7\", \" sieben\").replace(\"8\", \" acht\") \\\n",
        "            .replace(\"9\", \" neun\")\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        text = self.clean_http_urls.sub('', text)\n",
        "        text = self.clean_at_mentions.sub('', text)\n",
        "        text = self.replace_numbers(text)\n",
        "        text = self.clean_chars.sub('', text)  # use only text chars\n",
        "        text = ' '.join(text.split())  # substitute multiple whitespace with single whitespace\n",
        "        text = text.strip().lower()\n",
        "        return text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li40Xbc5ymPm"
      },
      "source": [
        "## Selbst geschriebene Pipeline \n",
        "(buggy (3 Warnings) und weiterhin in bearbeitung) Datarefiner(find_amplitude) funktioniert noch nicht\n",
        "\n",
        "**To do:**\n",
        "* cuda anpassen  (**DONE**)\n",
        "* Objektorientiert (**DONE**)\n",
        "* Logging (**DONE**)\n",
        "* testen um wieviel % Cuda schneller läuft\n",
        "* schauen warum \\*e zahlen dabei sind (*Können nun verarbeitet werden, ist also egal warum*) \n",
        "* max und mim Werte herausfinden   \n",
        "* durch die max werte Prozente implementieren \n",
        "* confidence berechnen die von BERT ausgegeben wird\n",
        "* Koorelation der labels erörtern\n",
        "* Vorfilter implementieren, welcher nach Keywords filtert\n",
        "* Dokumente in Sätze splitten und filtern\n",
        "* Boxplott erstellen, ggf. andere Visualisierungsmöglichkeiten ausprobieren\n",
        "* Visualizer Klasser erstellen\n",
        "* CSVHandler erweitern um mehr Dateiformate\n",
        "* auf git pushen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anHkWJUVVJ_G"
      },
      "source": [
        "***Documenter*** erstellt eine Logging Datei um besser nachvollziehen zu können wo ggf. Probleme entstehen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dXXhrXPVFQk"
      },
      "source": [
        "import logging\n",
        "\n",
        "# Creates a logging file for easy debugging and failure searching -> pipeline.log\n",
        "class Documenter:\n",
        "\n",
        "    def __init__(self):\n",
        "        logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', datefmt='%d/%m/%Y %H:%M:%S', # Date in German format, if desired change it\n",
        "                            filename='pipeline.log', filemode='w', level=logging.DEBUG)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vknki9bVtE8"
      },
      "source": [
        "***DataRefiner*** soll als säuberungs und auswertungs Klasse dienen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jag7_x1mV9aM"
      },
      "source": [
        "from typing import List\n",
        "import re\n",
        "import logging\n",
        "\n",
        "\n",
        "class DataRefiner:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.max_value_pos: float = 0  # pos = positive\n",
        "        self.min_value_pos: float = 0\n",
        "        self.max_value_neu: float = 0  # methods missing\n",
        "        self.min_value_neu: float = 0  # methods missing\n",
        "        self.max_value_neg: float = 0  # methods missing\n",
        "        self.min_value_neg: float = 0  # methods missing\n",
        "        self.max_value_title_pos: float = 0  # methods missing\n",
        "        self.min_value_title_pos: float = 0  # methods missing\n",
        "        self.max_value_title_neu: float = 0  # methods missing\n",
        "        self.min_value_title_neu: float = 0  # methods missing\n",
        "        self.max_value_title_neg: float = 0  # methods missing\n",
        "        self.min_value_title_neg: float = 0  # methods missing\n",
        "        self.line_count_raw_data = 0\n",
        "        self.line_count_raw_data_results = 0\n",
        "\n",
        "# Setter\n",
        "    def set_max_pos(self, value): #ggf. nicht benötigt\n",
        "        self.max_value_pos = value\n",
        "\n",
        "    def set_min_pos(self, value): #ggf. nicht benötigt\n",
        "        self.min_value_pos = value\n",
        "\n",
        "    def set_line_count_data(self, value):\n",
        "        self.line_count_raw_data = value\n",
        "\n",
        "    def set_line_count_result(self, value):\n",
        "        self.line_count_raw_data_results = value\n",
        "\n",
        "# Getter\n",
        "    def get_max_pos(self): #ggf. nicht benötigt\n",
        "        return self.max_value_pos\n",
        "\n",
        "    def get_min_pos(self): #ggf. nicht benötigt\n",
        "        return self.min_value_pos\n",
        "\n",
        "    def get_line_count_data(self):\n",
        "        return self.line_count_raw_data\n",
        "\n",
        "    def get_line_count_result(self):\n",
        "        return self.line_count_raw_data_results\n",
        "\n",
        "# Methods\n",
        "    # check if the amount of lines is equal between raw data and result\n",
        "    def check_line_count(self) -> bool:\n",
        "        if self.line_count_raw_data_results == self.line_count_raw_data:\n",
        "            logging.debug(\"Line count equal: continue computation\")\n",
        "            return True\n",
        "        else:\n",
        "            logging.debug(\"Line Count unequal: double check filenames and make sure the code is not interrupted\")\n",
        "            print_warning()\n",
        "            return False\n",
        "\n",
        "    # parses the SequenceClassifierObject into a 'clean' string // Edit from wiser David: NO it doesn't it returns a list in a list, need to fix\n",
        "    @staticmethod\n",
        "    def clear_logits(logits): # -> tuple[List, List]:\n",
        "        raw = str(logits).split('\\n')\n",
        "        clean_title_logits = re.findall('[-]?\\d+[.]\\d+|[-]?\\d+[.]\\d+e[+]*[-]*\\d', raw[0])\n",
        "        clean_text_logits = re.findall('[-]?\\d+[.]\\d+|[-]?\\d+[.]\\d+e[+]*[-]*\\d', raw[1])\n",
        "        return clean_title_logits, clean_text_logits\n",
        "\n",
        "    @staticmethod\n",
        "    def clear_value(value: [str]) -> List:\n",
        "        return (re.findall('[-]?\\d+[.]\\d+|[-]?\\d+[.]\\d+e[+]*[-]*\\d', value))\n",
        "\n",
        "    def find_amplitude(self, tensors: List[List[str]]):\n",
        "        for values in tensors:\n",
        "            value = values[0].split(',')\n",
        "            try:\n",
        "                for x in range(3):\n",
        "                    value[x] = self.clear_value(value[x])\n",
        "            except IndexError as error:\n",
        "                logging.error(\"Tensor values missing for title\", error)\n",
        "                print_warning()\n",
        "\n",
        "            if float(value[0][0]) > self.max_value_title_pos:          # clear_value gives back a list in a list, need to fix !! NOT WORKING SO FAR (REGEX macht mich fertig...)\n",
        "                self.max_value_title_pos = float(value[0][0])          # the matrices are a hack but it should work with it\n",
        "            if float(value[0][0]) < self.min_value_title_pos:\n",
        "                self.min_value_title_pos = float(value[0][0])\n",
        "            if float(value[1][0]) > self.max_value_title_neg:\n",
        "                self.max_value_title_neg = float(value[1][0])\n",
        "            if float(value[1[0]]) < self.min_value_title_neg:\n",
        "                self.min_value_title_neg = float(value[1][0])\n",
        "            if float(value[2][0]) > self.max_value_title_neu:\n",
        "                self.max_value_title_neu = float(value[2][0])\n",
        "            if float(value[2][0]) < self.min_value_title_neu:\n",
        "                self.min_value_title_neu = float(value[2][0])\n",
        "\n",
        "            value = values[1].split(',')\n",
        "            try:      # meckert wegen duplizierung, mir fällt aber im moment nicht ein wie man das anders lösen sollte da es eben nicht genau der gleiche code ist\n",
        "                for x in range(3):\n",
        "                    value[x] = self.clear_value(value[x])\n",
        "            except IndexError as error:\n",
        "                logging.error(\"Tensor values missing for body\", error)\n",
        "                print_warning()\n",
        "\n",
        "                if float(value[0][0]) > self.max_value_pos:\n",
        "                    self.max_value_pos = float(value[0][0])\n",
        "                if float(value[0][0]) < self.min_value_pos:\n",
        "                    self.min_value_pos = float(value[0][0])\n",
        "                if float(value[1][0]) > self.max_value_neg:\n",
        "                    self.max_value_neg = float(value[1][0])\n",
        "                if float(value[1][0]) < self.min_value_neg:\n",
        "                    self.min_value_neg = float(value[1][0])\n",
        "                if float(value[2][0]) > self.max_value_neu:\n",
        "                    self.max_value_neu = float(value[2][0])\n",
        "                if float(value[2][0]) < self.min_value_neu:\n",
        "                    self.min_value_neu = float(value[2][0])\n",
        "        logging.info(\"Found all amplitudes\")\n",
        "\n",
        "\n",
        "def print_warning():\n",
        "    print(\"Warning: check 'pipeline.log'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFpkH9jWXvD"
      },
      "source": [
        "Der ***CSVHandler*** liest und schreibt alle erforderlichen CSV Dateien (Dateiformate sollen erweitert werden) und sendet dem DataRefiner wichtige Meta-Daten.\n",
        "Maximalwerte werden nicht gemerkt, da diese auch ohne einlesen einer Rohdaten-CSV gefunden werden sollen. (Falls man die Tensor Werte schon gespeichert hat)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnd-6kJvWXEU"
      },
      "source": [
        "from typing import List\n",
        "import csv\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "\n",
        "class CsvHandler:\n",
        "\n",
        "    def __init__(self, datarefiner):\n",
        "        self.output = []\n",
        "        self.line_count = 0\n",
        "        self.row_count = 0\n",
        "        self.headline_row = 0\n",
        "        self.target_row = 0\n",
        "        self.id_row = 0\n",
        "        self.date_row = 0\n",
        "        self.refiner = datarefiner\n",
        "\n",
        "# Load a CSV file into a list\n",
        "# Mode will determine which kind of CSV is committed\n",
        "# Mode = 0 for the raw data CSV, Mode = 1 for the resulted CSV\n",
        "    def load_csv(self, source: [str], mode: [int]) -> List[List[str]]:\n",
        "\n",
        "        self.output = []\n",
        "        self.line_count = 0\n",
        "        self.row_count = 0\n",
        "        self.headline_row = 0\n",
        "        self.target_row = 0\n",
        "        self.id_row = 0\n",
        "        self.date_row = 0                                    # reset counter and output on every new method call\n",
        "\n",
        "        with open(source, encoding='utf-8') as csv_file:     # try to open CSV file\n",
        "            try:\n",
        "                csv_reader = csv.reader(csv_file, delimiter=';')\n",
        "            except csv.Error as error:\n",
        "                sys.exit(\"file {}, line {}: {}\".format(source, csv_reader.line_num, error))\n",
        "            logging.info(\"CSV successfully loaded\")\n",
        "\n",
        "            if mode == 0:\n",
        "                try:                                          # try to get the first line\n",
        "                    headline = csv_reader.__next__()\n",
        "                except csv.Error as error:\n",
        "                    sys.exit(\"file {}, line {}: {}\".format(source, csv_reader.line_num, error))\n",
        "\n",
        "                logging.info(\"Searching desired data\")\n",
        "                for x in headline:                            # find desired rows in CSV\n",
        "                    if x == '\"title\"' or x == 'title':\n",
        "                        self.headline_row = self.row_count\n",
        "                        self.row_count += 1\n",
        "                    elif x == '\"body\"' or x == 'body':\n",
        "                        self.target_row = self.row_count\n",
        "                        self.row_count += 1\n",
        "                    elif x == '\"id\"' or x == 'id':\n",
        "                        self.id_row = self.row_count\n",
        "                        self.row_count += 1\n",
        "                    elif x == '\"date\"' or x == 'date':\n",
        "                        self.date_row = self.row_count\n",
        "                        self.row_count += 1\n",
        "                    else:\n",
        "                        self.row_count += 1\n",
        "                logging.info(\"Desired data found\")\n",
        "\n",
        "                logging.info(\"Convert CSV into List\")                # clear data of unnecessary information\n",
        "                for row in csv_reader:\n",
        "                    if not row:                 # catch empty row\n",
        "                        continue\n",
        "                    self.output.append([row[self.id_row], row[self.date_row],\n",
        "                                        row[self.headline_row], row[self.target_row]])\n",
        "                    self.line_count += 1\n",
        "\n",
        "                logging.info(f\"Converting was successful: {self.line_count} Lines\")\n",
        "                self.refiner.set_line_count_data(self.line_count)  # push line_count to the DataRefiner Class\n",
        "                if not self.output:\n",
        "                    logging.error(\"Seems like your data CSV is empty, double check that CSV file has data in it\")\n",
        "                    sys.exit(\"data CSV file empty\")\n",
        "                else:\n",
        "                    return self.output\n",
        "\n",
        "            if mode == 1:\n",
        "                try:  # try to get the first line\n",
        "                    headline = csv_reader.__next__()\n",
        "                except csv.Error as error:\n",
        "                    sys.exit(\"file {}, line {}: {}\".format(source, csv_reader.line_num, error))\n",
        "\n",
        "                logging.info(\"Searching desired tensors\")\n",
        "                for x in headline:  # find desired rows in CSV\n",
        "                    if x == 'title':\n",
        "                        self.headline_row = self.row_count\n",
        "                        self.row_count += 1\n",
        "                    elif x == 'body':\n",
        "                        self.target_row = self.row_count\n",
        "                        self.row_count += 1\n",
        "                    else:\n",
        "                        self.row_count += 1\n",
        "                logging.info(\"Desired tensors found\")\n",
        "\n",
        "                logging.info(\"Convert CSV into List\")  # clear data of unnecessary information\n",
        "                for row in csv_reader:\n",
        "                    if not row:                         # catch empty row\n",
        "                        continue\n",
        "                    self.output.append([row[self.headline_row], row[self.target_row]])\n",
        "                    self.line_count += 1\n",
        "                logging.info(\"Tensors successfully converted\")\n",
        "\n",
        "                self.refiner.set_line_count_result(self.line_count)    # push line_count to the DataRefiner Class\n",
        "                if not self.output:\n",
        "                    logging.error(\"Seems like your result CSV is empty, double check that CSV file has data in it\")\n",
        "                    sys.exit(\"Result CSV file empty\")\n",
        "                else:\n",
        "                    return self.output\n",
        "\n",
        "            else:\n",
        "                logging.error(\"Wrong MODE in [load_csv]: Mode must be 0 or 1 {load_csv('Filename','Mode')}\")\n",
        "                sys.exit(\"Wrong Mode to load any CSV, check 'pipeline.log'\")\n",
        "\n",
        "# Create a CSV file for the results\n",
        "    @staticmethod\n",
        "    def create_result_csv(filename: [str]):\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as csv_file:\n",
        "                csv_writer = csv.writer(csv_file, delimiter=';')\n",
        "                csv_writer.writerow([\"id\", \"date\", \"title\", \"body\"])\n",
        "        except csv.Error as error:\n",
        "            logging.error(\"Could not create CSV file for results [create_result_csv]\")\n",
        "            sys.exit(\"file {}, line {}: {}\".format(filename, csv_writer, error))\n",
        "\n",
        "# Create a CSV file for the results\n",
        "    @staticmethod\n",
        "    def create_value_csv(filename: [str]):\n",
        "        try:\n",
        "            with open(filename, 'w', encoding='utf-8') as csv_file:\n",
        "                csv_writer = csv.writer(csv_file, delimiter=';')\n",
        "                csv_writer.writerow([\"max_positive\", \"min_positive\", \"max_neutral\",\n",
        "                                     \"min_neutral\", \"max_negative\", \"min_negative\"])\n",
        "        except csv.Error as error:\n",
        "            logging.error(\"Could not create CSV file for values [create_value_csv]\")\n",
        "            sys.exit(\"file {}, line {}: {}\".format(filename, csv_writer, error))\n",
        "\n",
        "# Write a List into a CSV file (appending)\n",
        "    @staticmethod\n",
        "    def write_result_csv(filename: [str], arg1: [str], arg2: [str], arg3: [str], arg4: [str]):\n",
        "        try:\n",
        "            with open(filename, 'a+', encoding='utf-8', newline='') as csv_file:\n",
        "                csv_writer = csv.writer(csv_file, delimiter=';')\n",
        "                csv_writer.writerow([arg1, arg2, arg3, arg4])\n",
        "        except csv.Error as error:\n",
        "            logging.error(\"Could not find CSV file for results [write_result_csv]\")\n",
        "            sys.exit(\"file {}, line {}: {}\".format(filename, csv_writer, error))\n",
        "        logging.info(\"Result saved\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdNEK9nyWxcd"
      },
      "source": [
        "Die ***Main***\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po05ygofsl9e"
      },
      "source": [
        "from germansentiment import SentimentModel\n",
        "#import csvhandler\n",
        "#import datarefiner\n",
        "#import documenter\n",
        "import logging\n",
        "\n",
        "# Initialisation\n",
        "doc = documenter.Documenter()       # Logging\n",
        "model = SentimentModel()            # Actual ML-Model by Oliver Guhr\n",
        "dr = datarefiner.DataRefiner()      # data refining\n",
        "csv = csvhandler.CsvHandler(dr)     # csv reader/writer; CSV_Handler must get a DataRefiner Object\n",
        "\n",
        "string_source = \"taz_filtered_christian_kahmann.csv\"\n",
        "string_results = \"results_1.csv\"\n",
        "string_test = \"test_2.csv\"\n",
        "\n",
        "logging.info(\"Model and classes loaded\")\n",
        "\n",
        "\n",
        "def write_data(information_list, filename):\n",
        "    for text in information_list:\n",
        "        article_id = text.pop(0)                # get article ID\n",
        "        date = text.pop(0)                     # get article date\n",
        "        result = model.predict_sentiment(text)  # call BERT\n",
        "\n",
        "        tensors_title, tensors_text = dr.clear_logits(result[0].logits)   # get tensor values\n",
        "        print(f\"Id: {article_id} /// Title: {tensors_title} /// Text: {tensors_text}\", end=': ')\n",
        "\n",
        "        csv.write_result_csv(filename, article_id, date, tensors_title, tensors_text)  # write output csv\n",
        "        # CSV schreiber muss wahrscheinlich noch von Liste in String geparsed werden\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    data = csv.load_csv(string_source, 0)                   # load CSV and convert into list\n",
        "    csv.create_result_csv(string_test)                  # create output CSV\n",
        "    write_data(data, string_test)                                     # Run BERT and write output CSV\n",
        "\n",
        "    results = csv.load_csv(string_test, 1)               # load result CSV and convert into list\n",
        "    print(dr.check_line_counts())                            # check if code ran smoothly so far\n",
        "    #dr.find_amplitude(results)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}